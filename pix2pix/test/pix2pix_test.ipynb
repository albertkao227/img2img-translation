{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430f55fb",
   "metadata": {},
   "source": [
    "# Pix2Pix \n",
    "\n",
    "1. generator\n",
    "- UNet\n",
    "2. discriminator \n",
    "- PatchGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55761513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a583251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7515d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet generator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "            ngf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "\n",
    "        We construct the U-Net from the innermost layer to the outermost layer.\n",
    "        It is a recursive process.\n",
    "        \"\"\"\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576353e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "\n",
    "netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "212b4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = r'..\\\\..\\\\..\\\\pix2pix\\\\pytorch-CycleGAN-and-pix2pix\\\\datasets\\\\facades'\n",
    "gpu_ids = 0 \n",
    "checkpoints_dir = '.'\n",
    "model = 'pix2pix'\n",
    "input_nc = int(3) # RGB \n",
    "output_nc = int(3)\n",
    "ngf = 64\n",
    "ndf = 64 \n",
    "\n",
    "netD = 'basic'\n",
    "n_layers_D = 3 \n",
    "init_type = 'normal'\n",
    "init_gain = 0.02 \n",
    "direction = 'AtoB'\n",
    "batch_size = 1\n",
    "load_size = 286\n",
    "crop_size = 256\n",
    "use_dropout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c6f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4747877f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cec0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9806226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512c9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(input_nc, output_nc, ngf, netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Create a generator\n",
    "\n",
    "    Parameters:\n",
    "        input_nc (int) -- the number of channels in input images\n",
    "        output_nc (int) -- the number of channels in output images\n",
    "        ngf (int) -- the number of filters in the last conv layer\n",
    "        netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128\n",
    "        norm (str) -- the name of normalization layers used in the network: batch | instance | none\n",
    "        use_dropout (bool) -- if use dropout layers.\n",
    "        init_type (str)    -- the name of our initialization method.\n",
    "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Returns a generator\n",
    "\n",
    "    Our current implementation provides two types of generators:\n",
    "        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)\n",
    "        The original U-Net paper: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\n",
    "        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\n",
    "        We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\n",
    "\n",
    "\n",
    "    The generator has been initialized by <init_net>. It uses RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netG == 'resnet_9blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n",
    "    elif netG == 'resnet_6blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n",
    "    elif netG == 'unet_128':\n",
    "        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    elif netG == 'unet_256':\n",
    "        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    else:\n",
    "        raise NotImplementedError('Generator model name [%s] is not recognized' % netG)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ee553",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def define_D(input_nc, ndf, netD, n_layers_D=3, norm='batch', init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Create a discriminator\n",
    "\n",
    "    Parameters:\n",
    "        input_nc (int)     -- the number of channels in input images\n",
    "        ndf (int)          -- the number of filters in the first conv layer\n",
    "        netD (str)         -- the architecture's name: basic | n_layers | pixel\n",
    "        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'\n",
    "        norm (str)         -- the type of normalization layers used in the network.\n",
    "        init_type (str)    -- the name of the initialization method.\n",
    "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Returns a discriminator\n",
    "\n",
    "    Our current implementation provides three types of discriminators:\n",
    "        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.\n",
    "        It can classify whether 70×70 overlapping patches are real or fake.\n",
    "        Such a patch-level discriminator architecture has fewer parameters\n",
    "        than a full-image discriminator and can work on arbitrarily-sized images\n",
    "        in a fully convolutional fashion.\n",
    "\n",
    "        [n_layers]: With this mode, you can specify the number of conv layers in the discriminator\n",
    "        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n",
    "\n",
    "        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n",
    "        It encourages greater color diversity but has no effect on spatial statistics.\n",
    "\n",
    "    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netD == 'basic':  # default PatchGAN classifier\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)\n",
    "    elif netD == 'n_layers':  # more options\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n",
    "    elif netD == 'pixel':     # classify if each pixel is real or fake\n",
    "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)\n",
    "    else:\n",
    "        raise NotImplementedError('Discriminator model name [%s] is not recognized' % netD)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3cd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixModel(BaseModel):\n",
    "    \"\"\" This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n",
    "\n",
    "    The model training requires '--dataset_mode aligned' dataset.\n",
    "    By default, it uses a '--netG unet256' U-Net generator,\n",
    "    a '--netD basic' discriminator (PatchGAN),\n",
    "    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n",
    "\n",
    "    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def modify_commandline_options(parser, is_train=True):\n",
    "        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n",
    "\n",
    "        Parameters:\n",
    "            parser          -- original option parser\n",
    "            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n",
    "\n",
    "        Returns:\n",
    "            the modified parser.\n",
    "\n",
    "        For pix2pix, we do not use image buffer\n",
    "        The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1\n",
    "        By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.\n",
    "        \"\"\"\n",
    "        # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)\n",
    "        parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')\n",
    "        if is_train:\n",
    "            parser.set_defaults(pool_size=0, gan_mode='vanilla')\n",
    "            parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for L1 loss')\n",
    "\n",
    "        return parser\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize the pix2pix class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        BaseModel.__init__(self, opt)\n",
    "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
    "        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']\n",
    "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
    "        self.visual_names = ['real_A', 'fake_B', 'real_B']\n",
    "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
    "        if self.isTrain:\n",
    "            self.model_names = ['G', 'D']\n",
    "        else:  # during test time, only load G\n",
    "            self.model_names = ['G']\n",
    "        # define networks (both generator and discriminator)\n",
    "        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n",
    "                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n",
    "            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n",
    "                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "        if self.isTrain:\n",
    "            # define loss functions\n",
    "            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)\n",
    "            self.criterionL1 = torch.nn.L1Loss()\n",
    "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
    "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D)\n",
    "\n",
    "    def set_input(self, input):\n",
    "        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
    "\n",
    "        Parameters:\n",
    "            input (dict): include the data itself and its metadata information.\n",
    "\n",
    "        The option 'direction' can be used to swap images in domain A and domain B.\n",
    "        \"\"\"\n",
    "        AtoB = self.opt.direction == 'AtoB'\n",
    "        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n",
    "        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n",
    "        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "        self.fake_B = self.netG(self.real_A)  # G(A)\n",
    "\n",
    "    def backward_D(self):\n",
    "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
    "        # Fake; stop backprop to the generator by detaching fake_B\n",
    "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
    "        pred_fake = self.netD(fake_AB.detach())\n",
    "        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "        # Real\n",
    "        real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
    "        pred_real = self.netD(real_AB)\n",
    "        self.loss_D_real = self.criterionGAN(pred_real, True)\n",
    "        # combine loss and calculate gradients\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"Calculate GAN and L1 loss for the generator\"\"\"\n",
    "        # First, G(A) should fake the discriminator\n",
    "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
    "        pred_fake = self.netD(fake_AB)\n",
    "        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
    "        # Second, G(A) = B\n",
    "        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
    "        # combine loss and calculate gradients\n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        self.forward()                   # compute fake images: G(A)\n",
    "        # update D\n",
    "        self.set_requires_grad(self.netD, True)  # enable backprop for D\n",
    "        self.optimizer_D.zero_grad()     # set D's gradients to zero\n",
    "        self.backward_D()                # calculate gradients for D\n",
    "        self.optimizer_D.step()          # update D's weights\n",
    "        # update G\n",
    "        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n",
    "        self.optimizer_G.zero_grad()        # set G's gradients to zero\n",
    "        self.backward_G()                   # calculate graidents for G\n",
    "        self.optimizer_G.step()             # update G's weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0aca97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1202a97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9785452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1d3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49bf129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
